---
title: "LLM"
date: 2025-09-10
type: diagram
tags:
  - llm
  - ai
  - mathematics
---

### The Core Process: How an LLM Generates Text

```mermaid
graph TD
    subgraph "User Input"
        A[Raw Text: &quot;The cat sat on the...&quot;]
    end

    subgraph "LLM Core Processing"
        B(Tokenization) --> C(Embeddings);
        
        subgraph "Inside the Transformer Block"
            direction TB
            D["<b>Input Embeddings</b><br/>+<br/><b>Positional Encoding</b><br/>(to understand word order)"];
            E["Self-Attention Mechanism<br/>(weighs word importance & context)"];
            F["Neural Network Layers<br/>(for deep processing)"];
            D --> E --> F;
        end
        
        C --> D;
        F --> G(Probability Calculation);
    end

    subgraph "Model Output"
        G --> H[Output Token: &quot;mat&quot;];
    end

    A --> B;

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

### The Learning Cycle: How an LLM Improves

```mermaid
graph TD
    A(Start With a Base Model) --> B["Make a Prediction"];
    B --> C["Calculate Error<br/>(Cost Function)"];
    C --> D["Adjust Model Parameters<br/>(Gradient Descent &<br/>Backpropagation)"];
    D --> B;

    subgraph "Guiding the Learning Process"
        E(Regularization Techniques<br/>e.g., Dropout) -- Prevents Overfitting --> D;
        F(Optimizers<br/>e.g., Adam) -- Improves Efficiency --> D;
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### From General Knowledge to Aligned Specialist

```mermaid
graph TD
    subgraph "Phase 1: Foundation Building"
        A["<b>Pretraining</b><br/>Model learns general<br/>language patterns,<br/>grammar, and facts from a<br/>massive, diverse dataset<br/>(the entire internet, books,<br/>etc.)."];
    end

    subgraph "Phase 2: Specialization"
        B["<b>Fine-Tuning/Transfer Learning</b><br/>The foundational model is<br/>adapted for a specific task<br/>(e.g., medical analysis,<br/>legal summaries) using a<br/>smaller, domain-specific<br/>dataset."];
    end

    subgraph "Phase 3: Alignment"
        C["<b>Reinforcement Learning<br/>from Human Feedback<br/>(RLHF)</b><br/>Humans rank model outputs<br/>for helpfulness and safety.<br/>This feedback trains the<br/>model to align its behavior<br/>with human values and<br/>expectations."];
    end

    subgraph "Result"
        D["A Specialized,<br/>Helpful, and Aligned LLM<br/>(e.g., ChatGPT, Gemini)"];
    end

    A --> B --> C --> D;
```

### Timeline of Key Breakthroughs

```mermaid
timeline

    1940s-1960s : Early Foundations
        : Claude Shannon's Information Theory (Language as probability)
        : ELIZA Chatbot (Pattern matching)

    1980s : Statistical Approaches
        : N-gram Models (Predicting words based on the last few words)

    2013 : The Meaning Revolution
        : Word2Vec (Representing word meaning as vectors/embeddings)

    2017 : The Architecture Breakthrough
        : "Attention Is All You Need" paper introduces the Transformer Architecture

    2020s-Present : The Age of Scale & Multimodality
        : Massive models (GPT series, Gemini) with trillions of parameters
        : Integration of text, images, and audio (Multimodality)
```

Source: [Mathematics of LLMs in Everyday Language](https://www.youtube.com/watch?v=1WHaFWMMXLI)
